# Postâ€‘Correction Relapse Rate

> A simple, traceâ€‘based metric for quantifying how often a session appears to recover â€” then drifts again.

---

## ðŸ§© What this metric captures

In multiâ€‘turn agents, a flow often **looks stable again** after a correction: tool arguments match constraints, responses resume the expected pattern, latencies return to normal.

But sometimes the stability is temporary â€” the session drifts again.

**Postâ€‘correction relapse rate** measures how often this happens.

This is one of the most reliable indicators that an agent has:

* shallow or inconsistent recovery behavior,
* unstable internal reasoning loops,
* or state that was â€œpatchedâ€ but not fully realigned.

---

## ðŸ“ Plainâ€‘language definition

> Of all sessions that show *at least one correction*, what fraction experience **another drift event** later in the same session?

This can be computed using only JSONL traces with minimal structure (trace_id, timestamps, event types).

---

## ðŸ“Š When to use this metric

Use this when you want to understand:

* whether your correction mechanisms are *actually stabilizing* behavior,
* whether user or agent corrections "stick,"
* whether loops, retries, or misalignment tend to recur,
* whether longer sessions correlate with higher recurrence.

It is especially helpful for comparing:

* different prompting strategies,
* different runtime policies,
* different agent architectures or tools.

---

## ðŸ“ Required trace structure

Minimal assumptions:

* each event has a **trace_id**,
* events appear in chronological order per trace,
* events include a **driftâ€‘like signal** and a **correctionâ€‘like signal**.

Examples of signals you could treat as drift:

* constraint violation,
* toolâ€‘call mismatch,
* missing required fields,
* user intent mismatch.

Examples of signals you could treat as correction:

* a retry with adjusted arguments,
* a clarifying question,
* a restated plan,
* an explicit repair pattern.

These definitions are intentionally flexible â€” the repo does not impose strict taxonomy.

---

## ðŸ”¢ Computation (from JSONL)

Pseudoâ€‘algorithm for traceâ€‘level computation:

```python
# For each session (grouped by trace_id):
had_correction = False
relapsed = False

for event in events:
    if is_correction(event):
        had_correction = True
    elif had_correction and is_drift(event):
        relapsed = True
        break  # further drift not needed

return had_correction, relapsed
```

Datasetâ€‘level rate:

```python
relapse_rate = (number_of_sessions_with_relapse) / (number_of_sessions_with_correction)
```

---

## ðŸ§  Interpretation

A **high relapse rate** suggests:

* corrections are shallow,
* internal state is not truly recovered,
* multiâ€‘turn reasoning continues drifting later,
* tools or retrieval introduce inconsistent signals,
* user reâ€‘direction is not strong enough.

A **low relapse rate** suggests:

* corrections reliably stabilize behavior,
* recovery patterns are strong,
* the agentâ€™s internal loop is resilient.

---

## ðŸ•³ï¸ Pitfalls & edge cases

* Sessions with **multiple corrections** may need stricter logic (e.g., count only first).
* Very short sessions inflate the â€œno relapseâ€ denominator.
* A correction followed by session end does **not** count as relapse.
* Longâ€‘horizon tasks tend to show higher relapse simply due to larger surface area.
* Toolâ€‘driven flows may show â€œsilent relapseâ€ in planning spans (detectable via latency or divergence).

---

## ðŸ“ˆ Optional extensions

Advanced variants you can compute:

### 1. **Relapse Turn Distance**

How many turns after correction does relapse occur?

### 2. **Severityâ€‘Weighted Relapse**

Weight later drift by strength/impact.

### 3. **Relapse Clusters**

Group sessions by where relapse tends to occur (early, mid, late).

These extensions integrate naturally with other metrics in this repo (e.g., recoveryâ€‘turnâ€‘distance, traceâ€‘treeâ€‘divergence).

---

## âœ… Summary

**Postâ€‘Correction Relapse Rate** is one of the simplest but most revealing metrics for understanding whether your correction mechanisms are truly stabilizing multiâ€‘turn agents.

It answers the practical question:

> "When we fix a session, does it actually stay fixed?"

---

If you'd like, I can also generate:

* plotting utilities,
* notebook examples,
* CLI integration for `compute_metrics_from_jsonl.py`.
